content='from fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nimport google.generativeai as genai\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nimport chromadb\nfrom chromadb.config import Settings\nimport os\nimport logging\nimport asyncio\nfrom contextlib import asynccontextmanager\nimport uvicorn\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Pydantic models\nclass ChatMessage(BaseModel):\n    role: str = Field(..., description="Role of the message sender (user/assistant)")\n    content: str = Field(..., description="Content of the message")\n\nclass ChatRequest(BaseModel):\n    message: str = Field(..., description="User message to send to Gemini")\n    history: Optional[List[ChatMessage]] = Field(default=[], description="Chat history")\n    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0)\n    max_tokens: Optional[int] = Field(default=1024, gt=0)\n\nclass ChatResponse(BaseModel):\n    response: str = Field(..., description="Gemini\'s response")\n    usage: Optional[Dict[str, Any]] = Field(default=None, description="Token usage information")\n\nclass EmbeddingRequest(BaseModel):\n    text: str = Field(..., description="Text to embed")\n    task_type: Optional[str] = Field(default="RETRIEVAL_DOCUMENT", description="Task type for embedding")\n\nclass EmbeddingResponse(BaseModel):\n    embedding: List[float] = Field(..., description="Generated embedding vector")\n    dimensions: int = Field(..., description="Number of dimensions in the embedding")\n\nclass DocumentRequest(BaseModel):\n    text: str = Field(..., description="Document text to store")\n    metadata: Optional[Dict[str, Any]] = Field(default={}, description="Document metadata")\n\nclass QueryRequest(BaseModel):\n    query: str = Field(..., description="Query text for similarity search")\n    n_results: Optional[int] = Field(default=5, gt=0, le=20, description="Number of results to return")\n\nclass SearchResult(BaseModel):\n    id: str = Field(..., description="Document ID")\n    text: str = Field(..., description="Document text")\n    metadata: Dict[str, Any] = Field(..., description="Document metadata")\n    distance: float = Field(..., description="Similarity distance")\n\nclass SearchResponse(BaseModel):\n    results: List[SearchResult] = Field(..., description="Search results")\n    query_embedding_dimensions: int = Field(..., description="Query embedding dimensions")\n\n# Global variables\ngemini_model = None\nembedding_model = None\nchroma_client = None\ncollection = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    """Application lifespan manager"""\n    global gemini_model, embedding_model, chroma_client, collection\n    \n    try:\n        # Initialize Gemini API\n        api_key = os.getenv("GEMINI_API_KEY")\n        if not api_key:\n            raise ValueError("GEMINI_API_KEY environment variable is required")\n        \n        genai.configure(api_key=api_key)\n        \n        # Initialize models with correct names\n        gemini_model = genai.GenerativeModel("gemini-2.0-flash-exp")\n        logger.info("Gemini chat model initialized successfully")\n        \n        # Initialize ChromaDB\n        chroma_client = chromadb.PersistentClient(\n            path="./chroma_db",\n            settings=Settings(anonymized_telemetry=False)\n        )\n        \n        # Create or get collection with correct embedding model\n        collection_name = "tutor_space_projects_3072"\n        try:\n            collection = chroma_client.get_collection(name=collection_name)\n            logger.info(f"Retrieved existing collection: {collection_name}")\n        except Exception:\n            collection = chroma_client.create_collection(\n                name=collection_name,\n                metadata={"hnsw:space' additional_kwargs={} response_metadata={'id': 'msg_01DnQwN1xi2N195uYACqd7sM', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 408, 'output_tokens': 1024, 'service_tier': 'standard'}} id='run--c69e4a85-de8b-4672-a12b-5b57e9d57f06-0' usage_metadata={'input_tokens': 408, 'output_tokens': 1024, 'total_tokens': 1432, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}