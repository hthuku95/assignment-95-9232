content='import pytest\nimport asyncio\nimport numpy as np\nfrom unittest.mock import Mock, patch, AsyncMock\nimport google.generativeai as genai\nfrom google.generativeai.types import GenerateContentResponse, EmbedContentResponse\nfrom google.ai.generativelanguage_v1beta.types import Content, Part\nimport os\nimport sys\nfrom typing import List, Dict, Any\n\n# Add the parent directory to the Python path to import project modules\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom src.models.gemini_client import GeminiClient\nfrom src.services.embedding_service import EmbeddingService\nfrom src.config.settings import Settings\n\n\nclass TestGeminiIntegration:\n    """Comprehensive tests for Gemini 2.5 Pro model integration."""\n    \n    @pytest.fixture\n    def mock_settings(self):\n        """Mock settings for testing."""\n        settings = Mock(spec=Settings)\n        settings.GEMINI_API_KEY = "test_api_key"\n        settings.GEMINI_MODEL = "gemini-2.5-pro"\n        settings.GEMINI_EMBEDDING_MODEL = "models/gemini-embedding-001"\n        settings.EMBEDDING_DIMENSION = 3072\n        settings.COLLECTION_NAME = "tutor_space_projects_3072"\n        return settings\n    \n    @pytest.fixture\n    def gemini_client(self, mock_settings):\n        """Create a GeminiClient instance for testing."""\n        with patch(\'google.generativeai.configure\') as mock_configure:\n            client = GeminiClient(mock_settings)\n            return client\n    \n    @pytest.fixture\n    def embedding_service(self, mock_settings):\n        """Create an EmbeddingService instance for testing."""\n        with patch(\'google.generativeai.configure\') as mock_configure:\n            service = EmbeddingService(mock_settings)\n            return service\n\n\nclass TestGeminiClient:\n    """Test the GeminiClient class with Gemini 2.5 Pro."""\n    \n    def test_client_initialization(self, gemini_client, mock_settings):\n        """Test that the client initializes with correct model name."""\n        assert gemini_client.model_name == "gemini-2.5-pro"\n        assert gemini_client.settings == mock_settings\n    \n    @patch(\'google.generativeai.GenerativeModel\')\n    def test_model_creation(self, mock_model_class, gemini_client):\n        """Test that the model is created with correct parameters."""\n        mock_model = Mock()\n        mock_model_class.return_value = mock_model\n        \n        model = gemini_client._get_model()\n        \n        mock_model_class.assert_called_once_with("gemini-2.5-pro")\n        assert model == mock_model\n    \n    @patch(\'google.generativeai.GenerativeModel\')\n    async def test_generate_content_success(self, mock_model_class, gemini_client):\n        """Test successful content generation."""\n        # Setup mocks\n        mock_model = Mock()\n        mock_response = Mock(spec=GenerateContentResponse)\n        mock_response.text = "Generated content response"\n        mock_model.generate_content_async = AsyncMock(return_value=mock_response)\n        mock_model_class.return_value = mock_model\n        \n        # Test\n        result = await gemini_client.generate_content("Test prompt")\n        \n        # Assertions\n        assert result == "Generated content response"\n        mock_model.generate_content_async.assert_called_once_with("Test prompt")\n    \n    @patch(\'google.generativeai.GenerativeModel\')\n    async def test_generate_content_with_system_instruction(self, mock_model_class, gemini_client):\n        """Test content generation with system instruction."""\n        mock_model = Mock()\n        mock_response = Mock(spec=GenerateContentResponse)\n        mock_response.text = "Response with system instruction"\n        mock_model.generate_content_async = AsyncMock(return_value=mock_response)\n        mock_model_class.return_value = mock_' additional_kwargs={} response_metadata={'id': 'msg_0184LdavMZAMfk5F7TyQXCEE', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 419, 'output_tokens': 1024, 'service_tier': 'standard'}} id='run--9f7e6f7a-a61e-4301-b907-5b2259ef716d-0' usage_metadata={'input_tokens': 419, 'output_tokens': 1024, 'total_tokens': 1443, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}