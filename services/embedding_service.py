content='import logging\nimport time\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nimport google.generativeai as genai\nfrom google.generativeai.types import EmbedContentResponse\nimport numpy as np\nimport os\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass EmbeddingResult:\n    """Result container for embedding operations"""\n    embedding: List[float]\n    model: str\n    task_type: str\n    dimensions: int\n    token_count: Optional[int] = None\n\nclass EmbeddingServiceError(Exception):\n    """Custom exception for embedding service errors"""\n    pass\n\nclass EmbeddingService:\n    """\n    Embedding service using Google Gemini models/gemini-embedding-001 with 3072 dimensions.\n    Supports both document and query embedding with proper task types.\n    """\n    \n    MODEL_NAME = "models/gemini-embedding-001"\n    EMBEDDING_DIMENSIONS = 3072\n    MAX_BATCH_SIZE = 100\n    MAX_TEXT_LENGTH = 2048\n    \n    # Task types for different use cases\n    TASK_RETRIEVAL_DOCUMENT = "RETRIEVAL_DOCUMENT"\n    TASK_RETRIEVAL_QUERY = "RETRIEVAL_QUERY"\n    TASK_SEMANTIC_SIMILARITY = "SEMANTIC_SIMILARITY"\n    TASK_CLASSIFICATION = "CLASSIFICATION"\n    TASK_CLUSTERING = "CLUSTERING"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        """\n        Initialize the embedding service.\n        \n        Args:\n            api_key: Google AI API key. If None, will try to get from environment\n        """\n        self.api_key = api_key or os.getenv(\'GOOGLE_AI_API_KEY\')\n        if not self.api_key:\n            raise EmbeddingServiceError("Google AI API key is required")\n        \n        # Configure the API\n        genai.configure(api_key=self.api_key)\n        \n        # Validate model availability\n        self._validate_model()\n        \n        logger.info(f"EmbeddingService initialized with model: {self.MODEL_NAME}")\n    \n    def _validate_model(self) -> None:\n        """Validate that the embedding model is available"""\n        try:\n            models = genai.list_models()\n            available_models = [model.name for model in models]\n            if self.MODEL_NAME not in available_models:\n                logger.warning(f"Model {self.MODEL_NAME} not found in available models")\n        except Exception as e:\n            logger.warning(f"Could not validate model availability: {e}")\n    \n    def _preprocess_text(self, text: str) -> str:\n        """\n        Preprocess text for embedding.\n        \n        Args:\n            text: Input text\n            \n        Returns:\n            Preprocessed text\n        """\n        if not text or not text.strip():\n            raise EmbeddingServiceError("Text cannot be empty")\n        \n        # Clean and truncate text\n        text = text.strip()\n        if len(text) > self.MAX_TEXT_LENGTH:\n            text = text[:self.MAX_TEXT_LENGTH]\n            logger.warning(f"Text truncated to {self.MAX_TEXT_LENGTH} characters")\n        \n        return text\n    \n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((Exception,))\n    )\n    def _generate_embedding(self, text: str, task_type: str) -> EmbedContentResponse:\n        """\n        Generate embedding with retry logic.\n        \n        Args:\n            text: Text to embed\n            task_type: Task type for the embedding\n            \n        Returns:\n            Embedding response from the API\n        """\n        try:\n            response = genai.embed_content(\n                model=self.MODEL_NAME,\n                content=text,\n                task_type' additional_kwargs={} response_metadata={'id': 'msg_01AWu1qnCZUqwFexEHwUhM4t', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 420, 'output_tokens': 1024, 'service_tier': 'standard'}} id='run--8a98f7de-d6c1-45de-8520-e0d6c726bd48-0' usage_metadata={'input_tokens': 420, 'output_tokens': 1024, 'total_tokens': 1444, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}