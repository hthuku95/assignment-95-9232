content='import os\nimport logging\nfrom typing import List, Dict, Any, Optional, Union\nimport google.generativeai as genai\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nimport time\nimport asyncio\nfrom dataclasses import dataclass\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass GeminiConfig:\n    """Configuration class for Gemini API settings"""\n    model_name: str = "gemini-2.5-pro"\n    embedding_model: str = "models/gemini-embedding-001"\n    max_retries: int = 3\n    retry_delay: float = 1.0\n    temperature: float = 0.7\n    top_p: float = 0.9\n    top_k: int = 40\n    max_output_tokens: int = 8192\n    embedding_dimensions: int = 3072\n\nclass GeminiServiceError(Exception):\n    """Custom exception for Gemini service errors"""\n    pass\n\nclass GeminiService:\n    """\n    Core Gemini API service with proper model configurations and error handling.\n    \n    This service provides methods for text generation, embeddings, and other\n    Gemini API functionalities with proper retry logic and error handling.\n    """\n    \n    def __init__(self, api_key: Optional[str] = None, config: Optional[GeminiConfig] = None):\n        """\n        Initialize the Gemini service.\n        \n        Args:\n            api_key: Google AI API key. If None, will use GOOGLE_AI_API_KEY environment variable\n            config: GeminiConfig object with service configuration\n        """\n        self.api_key = api_key or os.getenv(\'GOOGLE_AI_API_KEY\')\n        self.config = config or GeminiConfig()\n        \n        if not self.api_key:\n            raise GeminiServiceError("Google AI API key is required. Set GOOGLE_AI_API_KEY environment variable or pass api_key parameter.")\n        \n        # Configure the Gemini API\n        genai.configure(api_key=self.api_key)\n        \n        # Initialize models\n        self._initialize_models()\n        \n        logger.info(f"GeminiService initialized with model: {self.config.model_name}")\n    \n    def _initialize_models(self):\n        """Initialize Gemini models with proper configuration"""\n        try:\n            # Generation configuration\n            self.generation_config = genai.types.GenerationConfig(\n                temperature=self.config.temperature,\n                top_p=self.config.top_p,\n                top_k=self.config.top_k,\n                max_output_tokens=self.config.max_output_tokens,\n            )\n            \n            # Safety settings\n            self.safety_settings = {\n                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n            }\n            \n            # Initialize the generative model\n            self.model = genai.GenerativeModel(\n                model_name=self.config.model_name,\n                generation_config=self.generation_config,\n                safety_settings=self.safety_settings\n            )\n            \n            logger.info("Gemini models initialized successfully")\n            \n        except Exception as e:\n            logger.error(f"Failed to initialize Gemini models: {str(e)}")\n            raise GeminiServiceError(f"Model initialization failed: {str(e)}")\n    \n    def _retry_with_backoff(self, func, *args, **kwargs):\n        """\n        Execute function with exponential backoff retry logic.\n        \n        Args:\n            func: Function to execute\n            *args: Function arguments\n            **kwargs: Function keyword arguments\n            ' additional_kwargs={} response_metadata={'id': 'msg_01UgqGRJ26zEsAs65j8BKYtG', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 412, 'output_tokens': 1024, 'service_tier': 'standard'}} id='run--b22c5cff-f8dd-44e9-ae7d-d60c2cdd6a31-0' usage_metadata={'input_tokens': 412, 'output_tokens': 1024, 'total_tokens': 1436, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}