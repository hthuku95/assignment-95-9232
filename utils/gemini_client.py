content='import os\nimport logging\nfrom typing import List, Dict, Any, Optional, Union\nimport google.generativeai as genai\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom google.api_core import exceptions as google_exceptions\nimport time\n\nclass GeminiClient:\n    """\n    Google Gemini API client wrapper with proper authentication and error handling.\n    Supports text generation and embeddings with the correct model configurations.\n    """\n    \n    def __init__(self, api_key: Optional[str] = None):\n        """\n        Initialize the Gemini client with API key authentication.\n        \n        Args:\n            api_key: Google AI API key. If not provided, will try to get from environment.\n        """\n        self.api_key = api_key or os.getenv(\'GOOGLE_AI_API_KEY\') or os.getenv(\'GEMINI_API_KEY\')\n        if not self.api_key:\n            raise ValueError("Google AI API key is required. Set GOOGLE_AI_API_KEY or GEMINI_API_KEY environment variable.")\n        \n        # Configure the API\n        genai.configure(api_key=self.api_key)\n        \n        # Initialize models\n        self.text_model_name = "gemini-2.5-pro"\n        self.embedding_model_name = "models/gemini-embedding-001"\n        self.embedding_dimensions = 3072\n        \n        # Initialize the text generation model\n        self.text_model = genai.GenerativeModel(self.text_model_name)\n        \n        # Configure safety settings\n        self.safety_settings = {\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n        }\n        \n        # Generation configuration\n        self.generation_config = {\n            \'temperature\': 0.7,\n            \'top_p\': 0.8,\n            \'top_k\': 40,\n            \'max_output_tokens\': 8192,\n        }\n        \n        self.logger = logging.getLogger(__name__)\n    \n    def generate_text(self, prompt: str, **kwargs) -> str:\n        """\n        Generate text using the Gemini model.\n        \n        Args:\n            prompt: Input text prompt\n            **kwargs: Additional generation parameters\n            \n        Returns:\n            Generated text response\n            \n        Raises:\n            Exception: If the API call fails\n        """\n        try:\n            # Merge custom config with defaults\n            config = {**self.generation_config, **kwargs}\n            \n            response = self.text_model.generate_content(\n                prompt,\n                generation_config=config,\n                safety_settings=self.safety_settings\n            )\n            \n            if response.candidates:\n                return response.text\n            else:\n                self.logger.warning("No candidates returned from Gemini API")\n                return ""\n                \n        except google_exceptions.GoogleAPIError as e:\n            self.logger.error(f"Google API error in generate_text: {e}")\n            raise Exception(f"Gemini API error: {e}")\n        except Exception as e:\n            self.logger.error(f"Unexpected error in generate_text: {e}")\n            raise Exception(f"Text generation failed: {e}")\n    \n    def generate_embedding(self, text: str, task_type: str = "RETRIEVAL_DOCUMENT") -> List[float]:\n        """\n        Generate embeddings for text using the Gemini embedding model.\n        \n        Args:\n            text: Input text to embed\n            task_type: Task type for embedding ("RETRIEVAL_DOCUMENT" or "RETRIEVAL_QUERY")\n            \n        Returns:\n            List of embedding values (3072 dimensions)\n            \n        Raises:\n            Exception: If the API' additional_kwargs={} response_metadata={'id': 'msg_01G9TrE7fCC8HguZWi878Ycy', 'model': 'claude-sonnet-4-20250514', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 411, 'output_tokens': 1024, 'service_tier': 'standard'}} id='run--d977b8ce-6654-426b-9ab8-83c21b06edf6-0' usage_metadata={'input_tokens': 411, 'output_tokens': 1024, 'total_tokens': 1435, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}